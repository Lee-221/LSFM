import torch
import torch.nn as nn
import torch.nn.functional as F


# ==================== å›¾åƒç‰¹å¾å¢å¼ºæ¨¡å— (FMB) ====================
class ChannelMLP(nn.Module):
    """ä¸¤å±‚MLPè¿›è¡Œé€šé“é—´ä¿¡æ¯äº¤äº’"""

    def __init__(self, dim):
        super().__init__()
        self.linear1 = nn.Conv2d(dim, dim, 1, 1, 0)  # L1
        self.linear2 = nn.Conv2d(dim, dim, 1, 1, 0)  # L2

    def forward(self, x):
        # å…¬å¼(10)
        return self.linear2(torch.sigmoid(self.linear1(x)))


class ImageFeatureEnhancement(nn.Module):
    """å›¾åƒç‰¹å¾å¢å¼ºæ¨¡å— (3.2)"""

    def __init__(self, dim, down_scale=8, eps=1e-6):
        super().__init__()
        self.dim = dim
        self.down_scale = down_scale
        self.eps = eps

        # åŒåˆ†æ”¯ç»“æ„
        self.linear_split = nn.Conv2d(dim, dim * 2, 1, 1, 0)

        # ç©ºé—´åˆ†æ”¯ç»„ä»¶
        self.dw_conv = nn.Conv2d(dim, dim, 3, 1, 1, groups=dim)
        self.linear_w = nn.Conv2d(dim, dim, 1, 1, 0)
        self.alpha = nn.Parameter(torch.ones((1, dim, 1, 1)))
        self.beta = nn.Parameter(torch.zeros((1, dim, 1, 1)))

        # é€šé“åˆ†æ”¯ç»„ä»¶
        self.channel_mlp = ChannelMLP(dim)

        # èåˆçº¿æ€§å±‚
        self.linear_fuse = nn.Conv2d(dim, dim, 1, 1, 0)

    def forward(self, f):
        # L2èŒƒæ•°å½’ä¸€åŒ– (å…¬å¼1)
        norm = torch.norm(f, p=2, dim=1, keepdim=True)
        f_norm = f / (norm + self.eps)

        # åŒåˆ†æ”¯ç»“æ„å¤„ç†
        xy = self.linear_split(f_norm)
        x, y = xy.chunk(2, dim=1)  # F_x, F_y
        b, c, h, w = x.shape

        # ç¡®ä¿ h, w æ˜¯æ•´æ•°
        h_int, w_int = int(h), int(w)

        # ç©ºé—´åˆ†æ”¯å¤„ç† (å…¬å¼3-5)
        mu = torch.mean(x, dim=(-2, -1), keepdim=True)  # Î¼
        f_v = torch.mean((x - mu) ** 2, dim=(-2, -1), keepdim=True)  # F_v

        x_down = F.adaptive_max_pool2d(x, (h_int // self.down_scale, w_int // self.down_scale))
        f_s = self.dw_conv(x_down)

        w_tensor = torch.sigmoid(self.linear_w(self.alpha * f_s + self.beta * f_v))  # W
        w_up = F.interpolate(w_tensor, size=(h_int, w_int), mode='nearest')  # âœ… ä½¿ç”¨æ•´æ•°
        f_l = x * w_up  # å±€éƒ¨å¢å¼ºç‰¹å¾ (å…¬å¼5)

        # é€šé“åˆ†æ”¯å¤„ç† (å…¬å¼10)
        f_c = self.channel_mlp(y)

        # æ®‹å·®è¿æ¥èåˆ (å…¬å¼11-12)
        fused = self.linear_fuse(f_l + f_c)
        return fused + f  # æ®‹å·®è¿æ¥


# ==================== å¤šå°ºåº¦è½»é‡åŒ–ç‰¹å¾æå–æ¨¡å— ====================
class MultiScaleFeatureExtraction(nn.Module):
    """å¤šå°ºåº¦è½»é‡åŒ–ç‰¹å¾æå–æ¨¡å— (3.3)"""

    def __init__(self, dim, n_levels=4):
        super().__init__()
        self.n_levels = n_levels
        self.dim = dim
        chunk_dim = dim // n_levels

        # å¤šå°ºåº¦ç‰¹å¾æå– (å…¬å¼13)
        self.mfr = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(chunk_dim, chunk_dim, 3, 1, 1, groups=chunk_dim),  # æ·±åº¦å·ç§¯
                nn.Conv2d(chunk_dim, chunk_dim, 1),  # é€ç‚¹å·ç§¯
                nn.BatchNorm2d(chunk_dim)
            ) for _ in range(self.n_levels)])

        # ç‰¹å¾èåˆ
        self.aggr = nn.Sequential(
            nn.Conv2d(dim, dim, 1, 1, 0),
            nn.BatchNorm2d(dim)
        )
        self.act = nn.GELU()

        # åŠ¨æ€ç‰¹å¾èåˆç»„ä»¶ (å…¬å¼15-19)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv_atten = nn.Sequential(
            nn.Conv2d(dim * 2, dim * 2, 1, bias=False),
            nn.Sigmoid()
        )
        self.conv_redu = nn.Sequential(
            nn.Conv2d(dim * 2, dim * 2, 1, groups=dim * 2),
            nn.Conv2d(dim * 2, dim, 1),
            nn.BatchNorm2d(dim)
        )

        # ç©ºé—´æ³¨æ„åŠ›
        self.conv1 = nn.Sequential(
            nn.Conv2d(dim, dim, 1, 1, groups=dim, bias=True),
            nn.Conv2d(dim, 1, 1, 1, bias=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(dim, dim, 1, 1, groups=dim, bias=True),
            nn.Conv2d(dim, 1, 1, 1, bias=True)
        )
        self.nonlin = nn.Sigmoid()


    def forward_safm(self, x):
        """å¤šå°ºåº¦ç‰¹å¾æå– (å…¬å¼13-14)"""
        h, w = x.size()[-2:]
        h_int, w_int = int(h), int(w)  #è½¬æ¢ä¸ºæ•´æ•°
        xc = x.chunk(self.n_levels, dim=1)
        out = []

        for i in range(self.n_levels):
            if i > 0:  # ç¬¬i>0ä¸ªå°ºåº¦ç»„æ„å»ºé‡‘å­—å¡”ç»“æ„
                p_size = (h_int // 2 ** i, w_int // 2 ** i)
                s = F.adaptive_max_pool2d(xc[i], p_size)  # è‡ªé€‚åº”æœ€å¤§æ± åŒ–
                s = self.mfr[i](s)  # æ·±åº¦å¯åˆ†ç¦»å·ç§¯
                s = F.interpolate(s, size=(h_int, w_int), mode='bilinear')  # âœ… ä½¿ç”¨æ•´æ•°
            else:  # åŸºç¡€ç‰¹å¾ç»„ç›´æ¥æå–
                s = self.mfr[i](xc[i])
            out.append(s)

        out = self.aggr(torch.cat(out, dim=1))
        return self.act(out) * x  # ä¿ç•™åŸå§‹ç‰¹å¾ç»†èŠ‚

    def forward_dff(self, x, skip):
        """åŠ¨æ€ç‰¹å¾èåˆ (å…¬å¼15-19)"""
        output = torch.cat([x, skip], dim=1)

        # é€šé“æ³¨æ„åŠ›
        att = self.conv_atten(self.avg_pool(output))
        output = output * att
        output = self.conv_redu(output)

        # ç©ºé—´æ³¨æ„åŠ›
        att = self.conv1(x) + self.conv2(skip)
        att = self.nonlin(att)
        return output * att

    def forward(self, x, skip=None):
        x = self.forward_safm(x)
        if skip is not None:
            skip = self.forward_safm(skip)
            x = self.forward_dff(x, skip)
        return x


# ==================== è‡ªæ³¨æ„åŠ›æƒé‡åˆ†é…æ¨¡å— ====================
class SelfAttentionWeightAllocation(nn.Module):
    """è‡ªæ³¨æ„åŠ›æƒé‡åˆ†é…æ¨¡å— (å¯¹åº”æ–‡æ¡£3.4èŠ‚)"""

    def __init__(self, channels, factor=8):
        super().__init__()
        self.groups = factor
        assert channels // self.groups > 0

        self.softmax = nn.Softmax(-1)
        self.agp = nn.AdaptiveAvgPool2d((1, 1))
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.gn = nn.GroupNorm(channels // self.groups, channels // self.groups)
        self.conv1x1 = nn.Conv2d(channels // self.groups, channels // self.groups, 1, 1, 0)
        self.conv3x3 = nn.Conv2d(channels // self.groups, channels // self.groups, 3, 1, 1)

    def forward(self, x):
        # é€šé“åˆ†ç»„å¤„ç† (å…¬å¼20)
        b, c, h, w = x.size()
        h_int, w_int = int(h), int(w)  # ğŸ”¥ è½¬æ¢ä¸ºæ•´æ•°

        group_x = x.reshape(b * self.groups, -1, h_int, w_int)  # âœ… ä½¿ç”¨æ•´æ•°

        # ç©ºé—´æ–¹å‘åˆ†è§£ç­–ç•¥ (å…¬å¼21-23)
        x_h = self.pool_h(group_x)
        x_w = self.pool_w(group_x).permute(0, 1, 3, 2)
        hw = self.conv1x1(torch.cat([x_h, x_w], dim=2))
        x_h, x_w = torch.split(hw, [h_int, w_int], dim=2)  # âœ… ä½¿ç”¨æ•´æ•°

        # åŒå‘ç‰¹å¾äº¤äº’
        x1 = self.gn(group_x * x_h.sigmoid() * x_w.permute(0, 1, 3, 2).sigmoid())
        x2 = self.conv3x3(group_x)

        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        x11 = self.softmax(self.agp(x1).reshape(b * self.groups, -1, 1).permute(0, 2, 1))
        x12 = x2.reshape(b * self.groups, c // self.groups, -1)
        x21 = self.softmax(self.agp(x2).reshape(b * self.groups, -1, 1).permute(0, 2, 1))
        x22 = x1.reshape(b * self.groups, c // self.groups, -1)

        weights = (torch.matmul(x11, x12) + torch.matmul(x21, x22)).reshape(b * self.groups, 1, h, w)

        # æ®‹å·®è¿æ¥è¾“å‡º (å…¬å¼29)
        return (group_x * weights.sigmoid()).reshape(b, c, h, w) + x